{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import shuffle\n",
    "from joblib import dump, load\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModel, set_seed\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from joblib import dump\n",
    "import numpy as np\n",
    "import evaluate"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "set_seed(42)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ],
   "id": "7cf017deec5e8b51",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Load dataset",
   "id": "38ddbe1652243ecc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load a custom CSV file\n",
    "data_files = {\"train\": \"../../data/train_data.csv\", \"valid\":\"../../data/val_data.csv\",\"test\": \"../../data/test_data.csv\"}\n",
    "dataset = load_dataset(\"csv\", data_files=data_files)\n",
    "\n",
    "# Inspect the first few samples\n",
    "print(dataset[\"train\"][0])\n",
    "# Example: Mapping string labels to integers\n",
    "label_mapping = {\n",
    "    'notcb': 0,\n",
    "    'gender': 1,\n",
    "    'ethnicity': 2,\n",
    "    'religion': 3,\n",
    "    'age' : 4,\n",
    "    'other': 5\n",
    "}\n",
    "dataset = dataset.map(lambda x: {\"label\": label_mapping[x[\"label\"]]})\n",
    "\n",
    "# Verify the mapping\n",
    "print(dataset)"
   ],
   "id": "c8bc4d8e64815cb6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "model_name = \"roberta-base\"",
   "id": "b437a589f005fd0c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# tokenize",
   "id": "2409840e1038949d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "dataset[\"valid\"]",
   "id": "47c1d63470fa279",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_function(examples): #padding false becase of data collator later\n",
    "    return tokenizer(examples[\"text\"], padding=False, truncation=True, max_length=200)\n",
    "\n",
    "# Apply the tokenizer to the dataset\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Inspect tokenized samples\n",
    "print(tokenized_datasets[\"train\"][0])"
   ],
   "id": "18fdab83fc068487",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "metric = evaluate.load(\"recall\")\n",
    "\n",
    "\n",
    "def model_init():\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=6)\n",
    "    print(model.config)\n",
    "    return model"
   ],
   "id": "f3431f9f110537ef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# How to Custom Model",
   "id": "c31278a2718a044d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# import torch.nn as nn\n",
    "#\n",
    "# class CustomBERTModel(nn.Module):\n",
    "#     def __init__(self, pretrained_model_name, num_labels):\n",
    "#         super(CustomBERTModel, self).__init__()\n",
    "#         self.bert = AutoModel.from_pretrained(pretrained_model_name)\n",
    "#         self.dropout = nn.Dropout(0.3)\n",
    "#         self.fc = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "#\n",
    "#     def forward(self, input_ids, attention_mask):\n",
    "#         output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#         pooled_output = self.dropout(output[1])  # Applying dropout\n",
    "#         logits = self.fc(pooled_output)  # Adding a fully connected layer\n",
    "#         return logits\n",
    "#\n",
    "# # Initialize the custom model\n",
    "# custom_model = CustomBERTModel(\"bert-base-uncased\", num_labels=4)"
   ],
   "id": "fedecd23661ecd73",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## hyperparameter tuning",
   "id": "78c5ff94a71415a9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import optuna\n",
    "from optuna.storages import RDBStorage\n",
    "import os\n",
    "\n",
    "# Define persistent storage\n",
    "storage = RDBStorage(\"sqlite:///optuna_trials.db\")\n",
    "\n",
    "study = optuna.create_study(\n",
    "    study_name=\"roberta-opt-study\", direction=\"maximize\", storage=storage, load_if_exists=True\n",
    ")\n",
    "\n",
    "# set the wandb project where this run will be logged\n",
    "os.environ[\"WANDB_PROJECT\"]=\"cyberbullying-bert-based-finetuning\"\n",
    "\n",
    "# save your trained model checkpoint to wandb\n",
    "os.environ[\"WANDB_LOG_MODEL\"]=\"checkpoint\"\n",
    "\n",
    "# turn off watch to log faster\n",
    "os.environ[\"WANDB_WATCH\"]=\"false\"\n"
   ],
   "id": "61d1d9961e2c419e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ],
   "id": "6a2dbacd1aa24ca4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import wandb\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "\n",
    "    # Calculate precision, recall, f1 with macro averaging (treats all classes equally)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, predictions, average='macro', zero_division=0\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "def compute_objective(metrics):\n",
    "    return metrics[\"eval_recall\"]\n",
    "\n",
    "\n",
    "wandb.init(project=\"cyberbullying-bert-based-finetuning\", name=\"roberta-opt-study\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    logging_strategy=\"epoch\",\n",
    "    num_train_epochs=3,\n",
    "    report_to=\"wandb\",\n",
    "    logging_dir=\"./logs\",\n",
    "    run_name=f\"{model_name}-opt-study\",\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model_init=model_init,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"valid\"],\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ],
   "id": "70f462c5f9ac0135",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def optuna_hp_space(trial):\n",
    "    return {\n",
    "        \"learning_rate\" : trial.suggest_float(\"learning_rate\", 1e-5, 3e-5, log=True),\n",
    "        \"per_device_train_batch_size\" : trial.suggest_categorical(\"per_device_train_batch_size\", [16, 32, 64]),\n",
    "        \"num_train_epochs\" : trial.suggest_int(\"num_train_epochs\", 5, 10),\n",
    "        \"warmup_steps\" : trial.suggest_int(\"warmup_steps\", 0, 500),\n",
    "        \"weight_decay\" : trial.suggest_float(\"weight_decay\", 0.01, 0.1)\n",
    "}\n",
    "\n",
    "\n",
    "best_run = trainer.hyperparameter_search(\n",
    "    direction=\"maximize\",\n",
    "    backend=\"optuna\",\n",
    "    hp_space=optuna_hp_space,\n",
    "    n_trials=10,\n",
    "    compute_objective=compute_objective,\n",
    "    study_name=\"roberta-opt-study\",\n",
    "    storage=\"sqlite:///optuna_trials.db\",\n",
    "    load_if_exists=True,\n",
    ")\n",
    "\n",
    "print(best_run)"
   ],
   "id": "be6f743f91ff8f8b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import optuna\n",
    "from optuna.visualization.matplotlib import (\n",
    "    plot_optimization_history,\n",
    "    plot_intermediate_values,\n",
    "    plot_param_importances,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the study from RDB storage\n",
    "storage = optuna.storages.RDBStorage(\"sqlite:///optuna_trials.db\")\n",
    "\n",
    "study = optuna.load_study(study_name=\"roberta-opt-study\", storage=storage)\n",
    "\n",
    "# Plot optimization history\n",
    "ax1 = plot_optimization_history(study)\n",
    "plt.show()\n",
    "ax1.figure.savefig(\"optimization_history.png\")\n",
    "\n",
    "# Plot intermediate values (if using pruning and intermediate reports)\n",
    "ax2 = plot_intermediate_values(study)\n",
    "plt.show()\n",
    "ax2.figure.savefig(\"intermediate_values.png\")\n",
    "\n",
    "# Plot parameter importances\n",
    "ax3 = plot_param_importances(study)\n",
    "plt.show()\n",
    "ax3.figure.savefig(\"param_importances.png\")"
   ],
   "id": "12fa413328ce2cf9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training optimized",
   "id": "6be1a72cf89a009c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "#\n",
    "# # Define the model\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=6)\n",
    "#\n",
    "# # Load best hyperparameters (already defined earlier as best_hparams)\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./final_model\",\n",
    "#     learning_rate=best_hparams[\"learning_rate\"],\n",
    "#     per_device_train_batch_size=best_hparams[\"per_device_train_batch_size\"],\n",
    "#     weight_decay=best_hparams[\"weight_decay\"],\n",
    "#     eval_strategy=\"epoch\",\n",
    "#     save_strategy=\"epoch\",\n",
    "#     load_best_model_at_end=True,\n",
    "#     logging_strategy=\"epoch\",\n",
    "#     num_train_epochs=best_hparams[\"num_train_epochs\"],\n",
    "#     warmup_steps=best_hparams[\"warmup_steps\"],\n",
    "#     report_to=\"wandb\",\n",
    "#     run_name=\"final_run_with_best_hparams\",\n",
    "# )\n",
    "#\n",
    "# # Create Trainer\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=tokenized_datasets[\"train\"],\n",
    "#     eval_dataset=tokenized_datasets[\"valid\"],\n",
    "#     processing_class=tokenizer,\n",
    "#     data_collator=data_collator,\n",
    "#     compute_metrics=compute_metrics,\n",
    "# )\n",
    "# from transformers import EarlyStoppingCallback\n",
    "#\n",
    "# trainer.add_callback(EarlyStoppingCallback(early_stopping_patience=2))\n",
    "# # Train\n",
    "# trainer.train()\n",
    "#\n",
    "# # Save the model\n",
    "# trainer.save_model(\"./final_model\")"
   ],
   "id": "acd20815d3f43391",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# from matplotlib import pyplot as plt\n",
    "# from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "#\n",
    "# # Generate predictions\n",
    "# predictions = trainer.predict(tokenized_datasets[\"test\"])\n",
    "# predicted_labels = predictions.predictions.argmax(axis=-1)\n",
    "#\n",
    "# # Classification report\n",
    "# print(classification_report(tokenized_datasets[\"test\"][\"label\"], predicted_labels))\n",
    "#\n",
    "# # Confusion matrix\n",
    "# cm = confusion_matrix(tokenized_datasets[\"test\"][\"label\"], predicted_labels)\n",
    "# disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"nocb\", \"gender\",\"ethnicity\", \"religion\", \"age\",\"other\"])\n",
    "# disp.plot(cmap=\"Blues\")  # Optional: set a color map\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(\"confusion_matrix.png\", dpi=300)  # You can change the name or dpi as needed\n",
    "# plt.close()  # Close the plot to free memory if you're in a loop"
   ],
   "id": "619f53159586c9f1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# # Inspect misclassified samples\n",
    "# for idx, (pred, label) in enumerate(zip(predicted_labels, tokenized_datasets[\"test\"][\"label\"])):\n",
    "#     if pred != label:\n",
    "#         print(f\"Index: {idx}, Predicted: {pred}, Actual: {label}\")\n",
    "#         print(tokenized_datasets[\"test\"][idx][\"text\"])"
   ],
   "id": "7cd8ae0b19b75b95",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "tokenizer.save_pretrained(\"./final_model\")\n",
    "print(\"Model saved successfully!\")"
   ],
   "id": "c7ad36fbb39a963e",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
