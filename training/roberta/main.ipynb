{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-08T23:59:34.460729Z",
     "start_time": "2025-08-08T23:59:29.226167Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import shuffle\n",
    "from joblib import dump, load\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModel, set_seed\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from joblib import dump\n",
    "import numpy as np\n",
    "import evaluate"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SEBASTIAN\\PycharmProjects\\MachinePaper\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T00:16:55.884964Z",
     "start_time": "2025-08-09T00:16:55.879924Z"
    }
   },
   "cell_type": "code",
   "source": [
    "set_seed(42)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ],
   "id": "7cf017deec5e8b51",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Load dataset",
   "id": "38ddbe1652243ecc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-08T23:59:34.985620Z",
     "start_time": "2025-08-08T23:59:34.618242Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load a custom CSV file\n",
    "data_files = {\"train\": \"../../data/train_data.csv\", \"valid\":\"../../data/val_data.csv\",\"test\": \"../../data/test_data.csv\"}\n",
    "dataset = load_dataset(\"csv\", data_files=data_files)\n",
    "\n",
    "# Inspect the first few samples\n",
    "print(dataset[\"train\"][0])\n",
    "# Example: Mapping string labels to integers\n",
    "label_mapping = {\n",
    "    'notcb': 0,\n",
    "    'gender': 1,\n",
    "    'ethnicity': 2,\n",
    "    'religion': 3,\n",
    "    'age' : 4,\n",
    "    'other': 5\n",
    "}\n",
    "dataset = dataset.map(lambda x: {\"label\": label_mapping[x[\"label\"]]})\n",
    "\n",
    "# Verify the mapping\n",
    "print(dataset)"
   ],
   "id": "c8bc4d8e64815cb6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '[USER]: FUCK OBAMA, dumb ass nigger hahahaha you mad [HASHTAG] [HASHTAG]', 'label': 'ethnicity'}\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 32638\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 8160\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 7200\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-08T23:59:35.004210Z",
     "start_time": "2025-08-08T23:59:35.001115Z"
    }
   },
   "cell_type": "code",
   "source": "model_name = \"roberta-base\"",
   "id": "b437a589f005fd0c",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# tokenize",
   "id": "2409840e1038949d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-08T23:59:35.021111Z",
     "start_time": "2025-08-08T23:59:35.015975Z"
    }
   },
   "cell_type": "code",
   "source": "dataset[\"valid\"]",
   "id": "47c1d63470fa279",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 8160\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-08T23:59:37.170647Z",
     "start_time": "2025-08-08T23:59:35.225234Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize_function(examples): #padding false becase of data collator later\n",
    "    return tokenizer(examples[\"text\"], padding=False, truncation=True, max_length=200)\n",
    "\n",
    "# Apply the tokenizer to the dataset\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Inspect tokenized samples\n",
    "print(tokenized_datasets[\"train\"][0])"
   ],
   "id": "18fdab83fc068487",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 32638/32638 [00:00<00:00, 39006.16 examples/s]\n",
      "Map: 100%|██████████| 8160/8160 [00:00<00:00, 35352.09 examples/s]\n",
      "Map: 100%|██████████| 7200/7200 [00:00<00:00, 35715.16 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '[USER]: FUCK OBAMA, dumb ass nigger hahahaha you mad [HASHTAG] [HASHTAG]', 'label': 2, 'input_ids': [0, 10975, 47955, 42645, 46997, 28214, 19455, 6, 16881, 8446, 295, 41956, 2489, 298, 895, 11695, 47, 7758, 646, 725, 2336, 14469, 3450, 742, 646, 725, 2336, 14469, 3450, 742, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-08T23:59:37.989745Z",
     "start_time": "2025-08-08T23:59:37.190352Z"
    }
   },
   "cell_type": "code",
   "source": [
    "metric = evaluate.load(\"recall\")\n",
    "\n",
    "\n",
    "def model_init():\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=6)\n",
    "    print(model.config)\n",
    "    return model"
   ],
   "id": "f3431f9f110537ef",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# How to Custom Model",
   "id": "c31278a2718a044d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-08T23:59:38.046033Z",
     "start_time": "2025-08-08T23:59:38.042509Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import torch.nn as nn\n",
    "#\n",
    "# class CustomBERTModel(nn.Module):\n",
    "#     def __init__(self, pretrained_model_name, num_labels):\n",
    "#         super(CustomBERTModel, self).__init__()\n",
    "#         self.bert = AutoModel.from_pretrained(pretrained_model_name)\n",
    "#         self.dropout = nn.Dropout(0.3)\n",
    "#         self.fc = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "#\n",
    "#     def forward(self, input_ids, attention_mask):\n",
    "#         output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "#         pooled_output = self.dropout(output[1])  # Applying dropout\n",
    "#         logits = self.fc(pooled_output)  # Adding a fully connected layer\n",
    "#         return logits\n",
    "#\n",
    "# # Initialize the custom model\n",
    "# custom_model = CustomBERTModel(\"bert-base-uncased\", num_labels=4)"
   ],
   "id": "fedecd23661ecd73",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## hyperparameter tuning",
   "id": "78c5ff94a71415a9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T00:03:46.807210Z",
     "start_time": "2025-08-09T00:03:46.764100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import optuna\n",
    "from optuna.storages import RDBStorage\n",
    "import os\n",
    "\n",
    "# Define persistent storage\n",
    "storage = RDBStorage(\"sqlite:///optuna_trials.db\")\n",
    "\n",
    "study = optuna.create_study(\n",
    "    study_name=f\"{model_name}-opt-study\", direction=\"maximize\", storage=storage, load_if_exists=True\n",
    ")\n",
    "\n",
    "# set the wandb project where this run will be logged\n",
    "os.environ[\"WANDB_PROJECT\"]=\"cyberbullying-bert-based-finetuning\"\n",
    "\n",
    "# save your trained model checkpoint to wandb\n",
    "os.environ[\"WANDB_LOG_MODEL\"]=\"end\"\n",
    "\n",
    "# turn off watch to log faster\n",
    "os.environ[\"WANDB_WATCH\"]=\"false\"\n"
   ],
   "id": "61d1d9961e2c419e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-08 19:03:46,799] Using an existing study with name 'roberta-opt-study' instead of creating a new one.\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-08T23:59:38.485164Z",
     "start_time": "2025-08-08T23:59:38.482255Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ],
   "id": "6a2dbacd1aa24ca4",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T00:03:54.046886Z",
     "start_time": "2025-08-09T00:03:51.335900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import wandb\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "\n",
    "    # Calculate precision, recall, f1 with macro averaging (treats all classes equally)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, predictions, average='macro', zero_division=0\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "def compute_objective(metrics):\n",
    "    return metrics[\"eval_recall\"]\n",
    "\n",
    "\n",
    "wandb.init(project=\"cyberbullying-bert-based-finetuning\", name=f\"{model_name}-opt-study\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=3,\n",
    "    logging_strategy=\"epoch\",\n",
    "    num_train_epochs=3,\n",
    "    report_to=\"wandb\",\n",
    "    logging_dir=\"./logs\",\n",
    "    run_name=f\"{model_name}-opt-study\",\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model_init=model_init,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"valid\"],\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ],
   "id": "70f462c5f9ac0135",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">roberta-opt-study</strong> at: <a href='https://wandb.ai/flts123-utec-u-de-ingenier-a-tecnolog-a/cyberbullying-bert-based-finetuning/runs/meo5wum9' target=\"_blank\">https://wandb.ai/flts123-utec-u-de-ingenier-a-tecnolog-a/cyberbullying-bert-based-finetuning/runs/meo5wum9</a><br> View project at: <a href='https://wandb.ai/flts123-utec-u-de-ingenier-a-tecnolog-a/cyberbullying-bert-based-finetuning' target=\"_blank\">https://wandb.ai/flts123-utec-u-de-ingenier-a-tecnolog-a/cyberbullying-bert-based-finetuning</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250808_190136-meo5wum9\\logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\SEBASTIAN\\PycharmProjects\\MachinePaper\\training\\roberta\\wandb\\run-20250808_190351-pj6art7g</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/flts123-utec-u-de-ingenier-a-tecnolog-a/cyberbullying-bert-based-finetuning/runs/pj6art7g' target=\"_blank\">roberta-opt-study</a></strong> to <a href='https://wandb.ai/flts123-utec-u-de-ingenier-a-tecnolog-a/cyberbullying-bert-based-finetuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/flts123-utec-u-de-ingenier-a-tecnolog-a/cyberbullying-bert-based-finetuning' target=\"_blank\">https://wandb.ai/flts123-utec-u-de-ingenier-a-tecnolog-a/cyberbullying-bert-based-finetuning</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/flts123-utec-u-de-ingenier-a-tecnolog-a/cyberbullying-bert-based-finetuning/runs/pj6art7g' target=\"_blank\">https://wandb.ai/flts123-utec-u-de-ingenier-a-tecnolog-a/cyberbullying-bert-based-finetuning/runs/pj6art7g</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.54.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-09T00:19:59.728948Z",
     "start_time": "2025-08-09T00:19:25.361899Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def optuna_hp_space(trial):\n",
    "    return {\n",
    "        \"learning_rate\" : trial.suggest_float(\"learning_rate\", 1e-5, 3e-5, log=True),\n",
    "        \"per_device_train_batch_size\" : trial.suggest_categorical(\"per_device_train_batch_size\", [16, 32, 64]),\n",
    "        \"num_train_epochs\" : trial.suggest_int(\"num_train_epochs\", 5, 10),\n",
    "        \"warmup_steps\" : trial.suggest_int(\"warmup_steps\", 0, 500),\n",
    "        \"weight_decay\" : trial.suggest_float(\"weight_decay\", 0.01, 0.1)\n",
    "}\n",
    "\n",
    "\n",
    "best_run = trainer.hyperparameter_search(\n",
    "    direction=\"maximize\",\n",
    "    backend=\"optuna\",\n",
    "    hp_space=optuna_hp_space,\n",
    "    n_trials=10,\n",
    "    compute_objective=compute_objective,\n",
    "    study_name=f\"{model_name}-opt-study\",\n",
    "    storage=\"sqlite:///optuna_trials.db\",\n",
    "    load_if_exists=True,\n",
    ")\n",
    "\n",
    "print(best_run)"
   ],
   "id": "be6f743f91ff8f8b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-08 19:19:25,397] Using an existing study with name 'roberta-opt-study' instead of creating a new one.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobertaConfig {\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.54.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">magic-sun-8</strong> at: <a href='https://wandb.ai/flts123-utec-u-de-ingenier-a-tecnolog-a/cyberbullying-bert-based-finetuning/runs/1zq3h584' target=\"_blank\">https://wandb.ai/flts123-utec-u-de-ingenier-a-tecnolog-a/cyberbullying-bert-based-finetuning/runs/1zq3h584</a><br> View project at: <a href='https://wandb.ai/flts123-utec-u-de-ingenier-a-tecnolog-a/cyberbullying-bert-based-finetuning' target=\"_blank\">https://wandb.ai/flts123-utec-u-de-ingenier-a-tecnolog-a/cyberbullying-bert-based-finetuning</a><br>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250808_191717-1zq3h584\\logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\SEBASTIAN\\PycharmProjects\\MachinePaper\\training\\roberta\\wandb\\run-20250808_191927-s0tjbv6j</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/flts123-utec-u-de-ingenier-a-tecnolog-a/cyberbullying-bert-based-finetuning/runs/s0tjbv6j' target=\"_blank\">northern-wave-9</a></strong> to <a href='https://wandb.ai/flts123-utec-u-de-ingenier-a-tecnolog-a/cyberbullying-bert-based-finetuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/flts123-utec-u-de-ingenier-a-tecnolog-a/cyberbullying-bert-based-finetuning' target=\"_blank\">https://wandb.ai/flts123-utec-u-de-ingenier-a-tecnolog-a/cyberbullying-bert-based-finetuning</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/flts123-utec-u-de-ingenier-a-tecnolog-a/cyberbullying-bert-based-finetuning/runs/s0tjbv6j' target=\"_blank\">https://wandb.ai/flts123-utec-u-de-ingenier-a-tecnolog-a/cyberbullying-bert-based-finetuning/runs/s0tjbv6j</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SEBASTIAN\\PycharmProjects\\MachinePaper\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "C:\\Users\\SEBASTIAN\\PycharmProjects\\MachinePaper\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `RobertaSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='8160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   6/8160 00:23 < 13:11:44, 0.17 it/s, Epoch 0.00/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-08-08 19:19:59,063] Trial 2 failed with parameters: {'learning_rate': 1.3657379780940003e-05, 'per_device_train_batch_size': 32, 'num_train_epochs': 8, 'warmup_steps': 201, 'weight_decay': 0.08236181106735314} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\SEBASTIAN\\PycharmProjects\\MachinePaper\\.venv\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 201, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\SEBASTIAN\\PycharmProjects\\MachinePaper\\.venv\\Lib\\site-packages\\transformers\\integrations\\integration_utils.py\", line 265, in _objective\n",
      "    trainer.train(resume_from_checkpoint=checkpoint, trial=trial)\n",
      "    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\SEBASTIAN\\PycharmProjects\\MachinePaper\\.venv\\Lib\\site-packages\\transformers\\trainer.py\", line 2237, in train\n",
      "    return inner_training_loop(\n",
      "        args=args,\n",
      "    ...<2 lines>...\n",
      "        ignore_keys_for_eval=ignore_keys_for_eval,\n",
      "    )\n",
      "  File \"C:\\Users\\SEBASTIAN\\PycharmProjects\\MachinePaper\\.venv\\Lib\\site-packages\\transformers\\trainer.py\", line 2578, in _inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\n",
      "  File \"C:\\Users\\SEBASTIAN\\PycharmProjects\\MachinePaper\\.venv\\Lib\\site-packages\\transformers\\trainer.py\", line 3792, in training_step\n",
      "    loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)\n",
      "  File \"C:\\Users\\SEBASTIAN\\PycharmProjects\\MachinePaper\\.venv\\Lib\\site-packages\\transformers\\trainer.py\", line 3879, in compute_loss\n",
      "    outputs = model(**inputs)\n",
      "  File \"C:\\Users\\SEBASTIAN\\PycharmProjects\\MachinePaper\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\SEBASTIAN\\PycharmProjects\\MachinePaper\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1762, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"C:\\Users\\SEBASTIAN\\PycharmProjects\\MachinePaper\\.venv\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\", line 1216, in forward\n",
      "    outputs = self.roberta(\n",
      "        input_ids,\n",
      "    ...<7 lines>...\n",
      "        return_dict=return_dict,\n",
      "    )\n",
      "  File \"C:\\Users\\SEBASTIAN\\PycharmProjects\\MachinePaper\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\SEBASTIAN\\PycharmProjects\\MachinePaper\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1762, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"C:\\Users\\SEBASTIAN\\PycharmProjects\\MachinePaper\\.venv\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\", line 890, in forward\n",
      "    encoder_outputs = self.encoder(\n",
      "        embedding_output,\n",
      "    ...<9 lines>...\n",
      "        cache_position=cache_position,\n",
      "    )\n",
      "  File \"C:\\Users\\SEBASTIAN\\PycharmProjects\\MachinePaper\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\SEBASTIAN\\PycharmProjects\\MachinePaper\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1762, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"C:\\Users\\SEBASTIAN\\PycharmProjects\\MachinePaper\\.venv\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\", line 631, in forward\n",
      "    layer_outputs = layer_module(\n",
      "        hidden_states,\n",
      "    ...<6 lines>...\n",
      "        cache_position=cache_position,\n",
      "    )\n",
      "  File \"C:\\Users\\SEBASTIAN\\PycharmProjects\\MachinePaper\\.venv\\Lib\\site-packages\\transformers\\modeling_layers.py\", line 94, in __call__\n",
      "    return super().__call__(*args, **kwargs)\n",
      "           ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\SEBASTIAN\\PycharmProjects\\MachinePaper\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\SEBASTIAN\\PycharmProjects\\MachinePaper\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1762, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"C:\\Users\\SEBASTIAN\\PycharmProjects\\MachinePaper\\.venv\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\", line 539, in forward\n",
      "    self_attention_outputs = self.attention(\n",
      "        hidden_states,\n",
      "    ...<4 lines>...\n",
      "        cache_position=cache_position,\n",
      "    )\n",
      "  File \"C:\\Users\\SEBASTIAN\\PycharmProjects\\MachinePaper\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\SEBASTIAN\\PycharmProjects\\MachinePaper\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1762, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"C:\\Users\\SEBASTIAN\\PycharmProjects\\MachinePaper\\.venv\\Lib\\site-packages\\transformers\\utils\\deprecation.py\", line 172, in wrapped_func\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\SEBASTIAN\\PycharmProjects\\MachinePaper\\.venv\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\", line 466, in forward\n",
      "    self_outputs = self.self(\n",
      "        hidden_states,\n",
      "    ...<6 lines>...\n",
      "        cache_position=cache_position,\n",
      "    )\n",
      "  File \"C:\\Users\\SEBASTIAN\\PycharmProjects\\MachinePaper\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\SEBASTIAN\\PycharmProjects\\MachinePaper\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1762, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"C:\\Users\\SEBASTIAN\\PycharmProjects\\MachinePaper\\.venv\\Lib\\site-packages\\transformers\\utils\\deprecation.py\", line 172, in wrapped_func\n",
      "    return func(*args, **kwargs)\n",
      "  File \"C:\\Users\\SEBASTIAN\\PycharmProjects\\MachinePaper\\.venv\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\", line 352, in forward\n",
      "    self.key(current_states)\n",
      "    ~~~~~~~~^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\SEBASTIAN\\PycharmProjects\\MachinePaper\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1751, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\SEBASTIAN\\PycharmProjects\\MachinePaper\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1762, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"C:\\Users\\SEBASTIAN\\PycharmProjects\\MachinePaper\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py\", line 125, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "           ~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "[W 2025-08-08 19:19:59,069] Trial 2 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[18]\u001B[39m\u001B[32m, line 11\u001B[39m\n\u001B[32m      1\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34moptuna_hp_space\u001B[39m(trial):\n\u001B[32m      2\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m {\n\u001B[32m      3\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mlearning_rate\u001B[39m\u001B[33m\"\u001B[39m : trial.suggest_float(\u001B[33m\"\u001B[39m\u001B[33mlearning_rate\u001B[39m\u001B[33m\"\u001B[39m, \u001B[32m1e-5\u001B[39m, \u001B[32m3e-5\u001B[39m, log=\u001B[38;5;28;01mTrue\u001B[39;00m),\n\u001B[32m      4\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mper_device_train_batch_size\u001B[39m\u001B[33m\"\u001B[39m : trial.suggest_categorical(\u001B[33m\"\u001B[39m\u001B[33mper_device_train_batch_size\u001B[39m\u001B[33m\"\u001B[39m, [\u001B[32m16\u001B[39m, \u001B[32m32\u001B[39m, \u001B[32m64\u001B[39m]),\n\u001B[32m   (...)\u001B[39m\u001B[32m      7\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mweight_decay\u001B[39m\u001B[33m\"\u001B[39m : trial.suggest_float(\u001B[33m\"\u001B[39m\u001B[33mweight_decay\u001B[39m\u001B[33m\"\u001B[39m, \u001B[32m0.01\u001B[39m, \u001B[32m0.1\u001B[39m)\n\u001B[32m      8\u001B[39m }\n\u001B[32m---> \u001B[39m\u001B[32m11\u001B[39m best_run = \u001B[43mtrainer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mhyperparameter_search\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     12\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdirection\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mmaximize\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     13\u001B[39m \u001B[43m    \u001B[49m\u001B[43mbackend\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43moptuna\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     14\u001B[39m \u001B[43m    \u001B[49m\u001B[43mhp_space\u001B[49m\u001B[43m=\u001B[49m\u001B[43moptuna_hp_space\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     15\u001B[39m \u001B[43m    \u001B[49m\u001B[43mn_trials\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m10\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     16\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcompute_objective\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcompute_objective\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     17\u001B[39m \u001B[43m    \u001B[49m\u001B[43mstudy_name\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mroberta-opt-study\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     18\u001B[39m \u001B[43m    \u001B[49m\u001B[43mstorage\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43msqlite:///optuna_trials.db\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     19\u001B[39m \u001B[43m    \u001B[49m\u001B[43mload_if_exists\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m     20\u001B[39m \u001B[43m)\u001B[49m\n\u001B[32m     22\u001B[39m \u001B[38;5;28mprint\u001B[39m(best_run)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\MachinePaper\\.venv\\Lib\\site-packages\\transformers\\trainer.py:3670\u001B[39m, in \u001B[36mTrainer.hyperparameter_search\u001B[39m\u001B[34m(self, hp_space, compute_objective, n_trials, direction, backend, hp_name, **kwargs)\u001B[39m\n\u001B[32m   3667\u001B[39m \u001B[38;5;28mself\u001B[39m.hp_name = hp_name\n\u001B[32m   3668\u001B[39m \u001B[38;5;28mself\u001B[39m.compute_objective = default_compute_objective \u001B[38;5;28;01mif\u001B[39;00m compute_objective \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m compute_objective\n\u001B[32m-> \u001B[39m\u001B[32m3670\u001B[39m best_run = \u001B[43mbackend_obj\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_trials\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdirection\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   3672\u001B[39m \u001B[38;5;28mself\u001B[39m.hp_search_backend = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   3673\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m best_run\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\MachinePaper\\.venv\\Lib\\site-packages\\transformers\\hyperparameter_search.py:72\u001B[39m, in \u001B[36mOptunaBackend.run\u001B[39m\u001B[34m(self, trainer, n_trials, direction, **kwargs)\u001B[39m\n\u001B[32m     71\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mrun\u001B[39m(\u001B[38;5;28mself\u001B[39m, trainer, n_trials: \u001B[38;5;28mint\u001B[39m, direction: \u001B[38;5;28mstr\u001B[39m, **kwargs):\n\u001B[32m---> \u001B[39m\u001B[32m72\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mrun_hp_search_optuna\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrainer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_trials\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdirection\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\MachinePaper\\.venv\\Lib\\site-packages\\transformers\\integrations\\integration_utils.py:283\u001B[39m, in \u001B[36mrun_hp_search_optuna\u001B[39m\u001B[34m(trainer, n_trials, direction, **kwargs)\u001B[39m\n\u001B[32m    281\u001B[39m direction = \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01mif\u001B[39;00m directions \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m direction\n\u001B[32m    282\u001B[39m study = optuna.create_study(direction=direction, directions=directions, **kwargs)\n\u001B[32m--> \u001B[39m\u001B[32m283\u001B[39m \u001B[43mstudy\u001B[49m\u001B[43m.\u001B[49m\u001B[43moptimize\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_objective\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_trials\u001B[49m\u001B[43m=\u001B[49m\u001B[43mn_trials\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgc_after_trial\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgc_after_trial\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    284\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m study._is_multi_objective():\n\u001B[32m    285\u001B[39m     best_trial = study.best_trial\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\MachinePaper\\.venv\\Lib\\site-packages\\optuna\\study\\study.py:489\u001B[39m, in \u001B[36mStudy.optimize\u001B[39m\u001B[34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001B[39m\n\u001B[32m    387\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34moptimize\u001B[39m(\n\u001B[32m    388\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m    389\u001B[39m     func: ObjectiveFuncType,\n\u001B[32m   (...)\u001B[39m\u001B[32m    396\u001B[39m     show_progress_bar: \u001B[38;5;28mbool\u001B[39m = \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[32m    397\u001B[39m ) -> \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    398\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Optimize an objective function.\u001B[39;00m\n\u001B[32m    399\u001B[39m \n\u001B[32m    400\u001B[39m \u001B[33;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    487\u001B[39m \u001B[33;03m            If nested invocation of this method occurs.\u001B[39;00m\n\u001B[32m    488\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m489\u001B[39m     \u001B[43m_optimize\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    490\u001B[39m \u001B[43m        \u001B[49m\u001B[43mstudy\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    491\u001B[39m \u001B[43m        \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    492\u001B[39m \u001B[43m        \u001B[49m\u001B[43mn_trials\u001B[49m\u001B[43m=\u001B[49m\u001B[43mn_trials\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    493\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    494\u001B[39m \u001B[43m        \u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    495\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcatch\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mtuple\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcatch\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43misinstance\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mIterable\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mcatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    496\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    497\u001B[39m \u001B[43m        \u001B[49m\u001B[43mgc_after_trial\u001B[49m\u001B[43m=\u001B[49m\u001B[43mgc_after_trial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    498\u001B[39m \u001B[43m        \u001B[49m\u001B[43mshow_progress_bar\u001B[49m\u001B[43m=\u001B[49m\u001B[43mshow_progress_bar\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    499\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\MachinePaper\\.venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:64\u001B[39m, in \u001B[36m_optimize\u001B[39m\u001B[34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001B[39m\n\u001B[32m     62\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m     63\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m n_jobs == \u001B[32m1\u001B[39m:\n\u001B[32m---> \u001B[39m\u001B[32m64\u001B[39m         \u001B[43m_optimize_sequential\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     65\u001B[39m \u001B[43m            \u001B[49m\u001B[43mstudy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     66\u001B[39m \u001B[43m            \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     67\u001B[39m \u001B[43m            \u001B[49m\u001B[43mn_trials\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     68\u001B[39m \u001B[43m            \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     69\u001B[39m \u001B[43m            \u001B[49m\u001B[43mcatch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     70\u001B[39m \u001B[43m            \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     71\u001B[39m \u001B[43m            \u001B[49m\u001B[43mgc_after_trial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     72\u001B[39m \u001B[43m            \u001B[49m\u001B[43mreseed_sampler_rng\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m     73\u001B[39m \u001B[43m            \u001B[49m\u001B[43mtime_start\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m     74\u001B[39m \u001B[43m            \u001B[49m\u001B[43mprogress_bar\u001B[49m\u001B[43m=\u001B[49m\u001B[43mprogress_bar\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     75\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     76\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m     77\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m n_jobs == -\u001B[32m1\u001B[39m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\MachinePaper\\.venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:161\u001B[39m, in \u001B[36m_optimize_sequential\u001B[39m\u001B[34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001B[39m\n\u001B[32m    158\u001B[39m         \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[32m    160\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m161\u001B[39m     frozen_trial = \u001B[43m_run_trial\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstudy\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcatch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    162\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    163\u001B[39m     \u001B[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001B[39;00m\n\u001B[32m    164\u001B[39m     \u001B[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001B[39;00m\n\u001B[32m    165\u001B[39m     \u001B[38;5;66;03m# Please refer to the following PR for further details:\u001B[39;00m\n\u001B[32m    166\u001B[39m     \u001B[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001B[39;00m\n\u001B[32m    167\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m gc_after_trial:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\MachinePaper\\.venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:253\u001B[39m, in \u001B[36m_run_trial\u001B[39m\u001B[34m(study, func, catch)\u001B[39m\n\u001B[32m    246\u001B[39m         \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m, \u001B[33m\"\u001B[39m\u001B[33mShould not reach.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    248\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m    249\u001B[39m     frozen_trial.state == TrialState.FAIL\n\u001B[32m    250\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m func_err \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    251\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(func_err, catch)\n\u001B[32m    252\u001B[39m ):\n\u001B[32m--> \u001B[39m\u001B[32m253\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m func_err\n\u001B[32m    254\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m frozen_trial\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\MachinePaper\\.venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:201\u001B[39m, in \u001B[36m_run_trial\u001B[39m\u001B[34m(study, func, catch)\u001B[39m\n\u001B[32m    199\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m get_heartbeat_thread(trial._trial_id, study._storage):\n\u001B[32m    200\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m201\u001B[39m         value_or_values = \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    202\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m exceptions.TrialPruned \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    203\u001B[39m         \u001B[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001B[39;00m\n\u001B[32m    204\u001B[39m         state = TrialState.PRUNED\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\MachinePaper\\.venv\\Lib\\site-packages\\transformers\\integrations\\integration_utils.py:265\u001B[39m, in \u001B[36mrun_hp_search_optuna.<locals>._objective\u001B[39m\u001B[34m(trial, checkpoint_dir)\u001B[39m\n\u001B[32m    263\u001B[39m     trainer.train(resume_from_checkpoint=checkpoint, trial=trial)\n\u001B[32m    264\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m265\u001B[39m     \u001B[43mtrainer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcheckpoint\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    266\u001B[39m \u001B[38;5;66;03m# If there hasn't been any evaluation during the training loop.\u001B[39;00m\n\u001B[32m    267\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(trainer, \u001B[33m\"\u001B[39m\u001B[33mobjective\u001B[39m\u001B[33m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\MachinePaper\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2237\u001B[39m, in \u001B[36mTrainer.train\u001B[39m\u001B[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[39m\n\u001B[32m   2235\u001B[39m         hf_hub_utils.enable_progress_bars()\n\u001B[32m   2236\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m2237\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43minner_training_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   2238\u001B[39m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[43m=\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2239\u001B[39m \u001B[43m        \u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m=\u001B[49m\u001B[43mresume_from_checkpoint\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2240\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2241\u001B[39m \u001B[43m        \u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m=\u001B[49m\u001B[43mignore_keys_for_eval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   2242\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\MachinePaper\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2578\u001B[39m, in \u001B[36mTrainer._inner_training_loop\u001B[39m\u001B[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[39m\n\u001B[32m   2571\u001B[39m context = (\n\u001B[32m   2572\u001B[39m     functools.partial(\u001B[38;5;28mself\u001B[39m.accelerator.no_sync, model=model)\n\u001B[32m   2573\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m i != \u001B[38;5;28mlen\u001B[39m(batch_samples) - \u001B[32m1\u001B[39m\n\u001B[32m   2574\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001B[32m   2575\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m contextlib.nullcontext\n\u001B[32m   2576\u001B[39m )\n\u001B[32m   2577\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m context():\n\u001B[32m-> \u001B[39m\u001B[32m2578\u001B[39m     tr_loss_step = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtraining_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_items_in_batch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2580\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m   2581\u001B[39m     args.logging_nan_inf_filter\n\u001B[32m   2582\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torch_xla_available()\n\u001B[32m   2583\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m (torch.isnan(tr_loss_step) \u001B[38;5;129;01mor\u001B[39;00m torch.isinf(tr_loss_step))\n\u001B[32m   2584\u001B[39m ):\n\u001B[32m   2585\u001B[39m     \u001B[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001B[39;00m\n\u001B[32m   2586\u001B[39m     tr_loss = tr_loss + tr_loss / (\u001B[32m1\u001B[39m + \u001B[38;5;28mself\u001B[39m.state.global_step - \u001B[38;5;28mself\u001B[39m._globalstep_last_logged)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\MachinePaper\\.venv\\Lib\\site-packages\\transformers\\trainer.py:3792\u001B[39m, in \u001B[36mTrainer.training_step\u001B[39m\u001B[34m(self, model, inputs, num_items_in_batch)\u001B[39m\n\u001B[32m   3789\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m loss_mb.reduce_mean().detach().to(\u001B[38;5;28mself\u001B[39m.args.device)\n\u001B[32m   3791\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m.compute_loss_context_manager():\n\u001B[32m-> \u001B[39m\u001B[32m3792\u001B[39m     loss = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mcompute_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_items_in_batch\u001B[49m\u001B[43m=\u001B[49m\u001B[43mnum_items_in_batch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   3794\u001B[39m \u001B[38;5;28;01mdel\u001B[39;00m inputs\n\u001B[32m   3795\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m   3796\u001B[39m     \u001B[38;5;28mself\u001B[39m.args.torch_empty_cache_steps \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   3797\u001B[39m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m.state.global_step % \u001B[38;5;28mself\u001B[39m.args.torch_empty_cache_steps == \u001B[32m0\u001B[39m\n\u001B[32m   3798\u001B[39m ):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\MachinePaper\\.venv\\Lib\\site-packages\\transformers\\trainer.py:3879\u001B[39m, in \u001B[36mTrainer.compute_loss\u001B[39m\u001B[34m(self, model, inputs, return_outputs, num_items_in_batch)\u001B[39m\n\u001B[32m   3877\u001B[39m         kwargs[\u001B[33m\"\u001B[39m\u001B[33mnum_items_in_batch\u001B[39m\u001B[33m\"\u001B[39m] = num_items_in_batch\n\u001B[32m   3878\u001B[39m     inputs = {**inputs, **kwargs}\n\u001B[32m-> \u001B[39m\u001B[32m3879\u001B[39m outputs = \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   3880\u001B[39m \u001B[38;5;66;03m# Save past state if it exists\u001B[39;00m\n\u001B[32m   3881\u001B[39m \u001B[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001B[39;00m\n\u001B[32m   3882\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.args.past_index >= \u001B[32m0\u001B[39m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\MachinePaper\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\MachinePaper\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\MachinePaper\\.venv\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:1216\u001B[39m, in \u001B[36mRobertaForSequenceClassification.forward\u001B[39m\u001B[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001B[39m\n\u001B[32m   1199\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33mr\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m   1200\u001B[39m \u001B[33;03mtoken_type_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001B[39;00m\n\u001B[32m   1201\u001B[39m \u001B[33;03m    Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,1]`:\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m   1212\u001B[39m \u001B[33;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001B[39;00m\n\u001B[32m   1213\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m   1214\u001B[39m return_dict = return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m.config.use_return_dict\n\u001B[32m-> \u001B[39m\u001B[32m1216\u001B[39m outputs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mroberta\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1217\u001B[39m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1218\u001B[39m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1219\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtoken_type_ids\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtoken_type_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1220\u001B[39m \u001B[43m    \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[43m=\u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1221\u001B[39m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1222\u001B[39m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[43m=\u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1223\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1224\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1225\u001B[39m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1226\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1227\u001B[39m sequence_output = outputs[\u001B[32m0\u001B[39m]\n\u001B[32m   1228\u001B[39m logits = \u001B[38;5;28mself\u001B[39m.classifier(sequence_output)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\MachinePaper\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\MachinePaper\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\MachinePaper\\.venv\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:890\u001B[39m, in \u001B[36mRobertaModel.forward\u001B[39m\u001B[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001B[39m\n\u001B[32m    883\u001B[39m \u001B[38;5;66;03m# Prepare head mask if needed\u001B[39;00m\n\u001B[32m    884\u001B[39m \u001B[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001B[39;00m\n\u001B[32m    885\u001B[39m \u001B[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001B[39;00m\n\u001B[32m    886\u001B[39m \u001B[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001B[39;00m\n\u001B[32m    887\u001B[39m \u001B[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001B[39;00m\n\u001B[32m    888\u001B[39m head_mask = \u001B[38;5;28mself\u001B[39m.get_head_mask(head_mask, \u001B[38;5;28mself\u001B[39m.config.num_hidden_layers)\n\u001B[32m--> \u001B[39m\u001B[32m890\u001B[39m encoder_outputs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mencoder\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    891\u001B[39m \u001B[43m    \u001B[49m\u001B[43membedding_output\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    892\u001B[39m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mextended_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    893\u001B[39m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    894\u001B[39m \u001B[43m    \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m=\u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    895\u001B[39m \u001B[43m    \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mencoder_extended_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    896\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    897\u001B[39m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m=\u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    898\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    899\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    900\u001B[39m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m=\u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    901\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcache_position\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcache_position\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    902\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    903\u001B[39m sequence_output = encoder_outputs[\u001B[32m0\u001B[39m]\n\u001B[32m    904\u001B[39m pooled_output = \u001B[38;5;28mself\u001B[39m.pooler(sequence_output) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.pooler \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\MachinePaper\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\MachinePaper\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\MachinePaper\\.venv\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:631\u001B[39m, in \u001B[36mRobertaEncoder.forward\u001B[39m\u001B[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001B[39m\n\u001B[32m    627\u001B[39m     all_hidden_states = all_hidden_states + (hidden_states,)\n\u001B[32m    629\u001B[39m layer_head_mask = head_mask[i] \u001B[38;5;28;01mif\u001B[39;00m head_mask \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m631\u001B[39m layer_outputs = \u001B[43mlayer_module\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    632\u001B[39m \u001B[43m    \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    633\u001B[39m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    634\u001B[39m \u001B[43m    \u001B[49m\u001B[43mlayer_head_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    635\u001B[39m \u001B[43m    \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# as a positional argument for gradient checkpointing\u001B[39;49;00m\n\u001B[32m    636\u001B[39m \u001B[43m    \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    637\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    638\u001B[39m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    639\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcache_position\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcache_position\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    640\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    642\u001B[39m hidden_states = layer_outputs[\u001B[32m0\u001B[39m]\n\u001B[32m    643\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m output_attentions:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\MachinePaper\\.venv\\Lib\\site-packages\\transformers\\modeling_layers.py:94\u001B[39m, in \u001B[36mGradientCheckpointingLayer.__call__\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m     91\u001B[39m         logger.warning(message)\n\u001B[32m     93\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._gradient_checkpointing_func(partial(\u001B[38;5;28msuper\u001B[39m().\u001B[34m__call__\u001B[39m, **kwargs), *args)\n\u001B[32m---> \u001B[39m\u001B[32m94\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[34;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\MachinePaper\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\MachinePaper\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\MachinePaper\\.venv\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:539\u001B[39m, in \u001B[36mRobertaLayer.forward\u001B[39m\u001B[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions, cache_position)\u001B[39m\n\u001B[32m    528\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\n\u001B[32m    529\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m    530\u001B[39m     hidden_states: torch.Tensor,\n\u001B[32m   (...)\u001B[39m\u001B[32m    537\u001B[39m     cache_position: Optional[torch.Tensor] = \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m    538\u001B[39m ) -> \u001B[38;5;28mtuple\u001B[39m[torch.Tensor]:\n\u001B[32m--> \u001B[39m\u001B[32m539\u001B[39m     self_attention_outputs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mattention\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    540\u001B[39m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    541\u001B[39m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    542\u001B[39m \u001B[43m        \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    543\u001B[39m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    544\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    545\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcache_position\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcache_position\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    546\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    547\u001B[39m     attention_output = self_attention_outputs[\u001B[32m0\u001B[39m]\n\u001B[32m    548\u001B[39m     outputs = self_attention_outputs[\u001B[32m1\u001B[39m:]  \u001B[38;5;66;03m# add self attentions if we output attention weights\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\MachinePaper\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\MachinePaper\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\MachinePaper\\.venv\\Lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001B[39m, in \u001B[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    168\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m minimum_action \u001B[38;5;129;01min\u001B[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torchdynamo_compiling():\n\u001B[32m    169\u001B[39m     \u001B[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001B[39;00m\n\u001B[32m    170\u001B[39m     warnings.warn(message, \u001B[38;5;167;01mFutureWarning\u001B[39;00m, stacklevel=\u001B[32m2\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m172\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\MachinePaper\\.venv\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:466\u001B[39m, in \u001B[36mRobertaAttention.forward\u001B[39m\u001B[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions, cache_position)\u001B[39m\n\u001B[32m    454\u001B[39m \u001B[38;5;129m@deprecate_kwarg\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mencoder_attention_mask\u001B[39m\u001B[33m\"\u001B[39m, version=\u001B[33m\"\u001B[39m\u001B[33m4.55.0\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m    455\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\n\u001B[32m    456\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m    464\u001B[39m     cache_position: Optional[torch.Tensor] = \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m    465\u001B[39m ) -> \u001B[38;5;28mtuple\u001B[39m[torch.Tensor]:\n\u001B[32m--> \u001B[39m\u001B[32m466\u001B[39m     self_outputs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mself\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    467\u001B[39m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    468\u001B[39m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    469\u001B[39m \u001B[43m        \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    470\u001B[39m \u001B[43m        \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m=\u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    471\u001B[39m \u001B[43m        \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    472\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    473\u001B[39m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m=\u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    474\u001B[39m \u001B[43m        \u001B[49m\u001B[43mcache_position\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcache_position\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    475\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    476\u001B[39m     attention_output = \u001B[38;5;28mself\u001B[39m.output(self_outputs[\u001B[32m0\u001B[39m], hidden_states)\n\u001B[32m    477\u001B[39m     outputs = (attention_output,) + self_outputs[\u001B[32m1\u001B[39m:]  \u001B[38;5;66;03m# add attentions if we output them\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\MachinePaper\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\MachinePaper\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\MachinePaper\\.venv\\Lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001B[39m, in \u001B[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    168\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m minimum_action \u001B[38;5;129;01min\u001B[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torchdynamo_compiling():\n\u001B[32m    169\u001B[39m     \u001B[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001B[39;00m\n\u001B[32m    170\u001B[39m     warnings.warn(message, \u001B[38;5;167;01mFutureWarning\u001B[39;00m, stacklevel=\u001B[32m2\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m172\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\MachinePaper\\.venv\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:352\u001B[39m, in \u001B[36mRobertaSdpaSelfAttention.forward\u001B[39m\u001B[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions, cache_position)\u001B[39m\n\u001B[32m    349\u001B[39m     value_layer = curr_past_key_value.value_cache[\u001B[38;5;28mself\u001B[39m.layer_idx]\n\u001B[32m    350\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    351\u001B[39m     key_layer = (\n\u001B[32m--> \u001B[39m\u001B[32m352\u001B[39m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcurrent_states\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    353\u001B[39m         .view(bsz, -\u001B[32m1\u001B[39m, \u001B[38;5;28mself\u001B[39m.num_attention_heads, \u001B[38;5;28mself\u001B[39m.attention_head_size)\n\u001B[32m    354\u001B[39m         .transpose(\u001B[32m1\u001B[39m, \u001B[32m2\u001B[39m)\n\u001B[32m    355\u001B[39m     )\n\u001B[32m    356\u001B[39m     value_layer = (\n\u001B[32m    357\u001B[39m         \u001B[38;5;28mself\u001B[39m.value(current_states)\n\u001B[32m    358\u001B[39m         .view(bsz, -\u001B[32m1\u001B[39m, \u001B[38;5;28mself\u001B[39m.num_attention_heads, \u001B[38;5;28mself\u001B[39m.attention_head_size)\n\u001B[32m    359\u001B[39m         .transpose(\u001B[32m1\u001B[39m, \u001B[32m2\u001B[39m)\n\u001B[32m    360\u001B[39m     )\n\u001B[32m    362\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m past_key_value \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    363\u001B[39m         \u001B[38;5;66;03m# save all key/value_layer to cache to be re-used for fast auto-regressive generation\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\MachinePaper\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1749\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1750\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1751\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\MachinePaper\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1757\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1758\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1759\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1760\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1761\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1762\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1764\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1765\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\MachinePaper\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001B[39m, in \u001B[36mLinear.forward\u001B[39m\u001B[34m(self, input)\u001B[39m\n\u001B[32m    124\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) -> Tensor:\n\u001B[32m--> \u001B[39m\u001B[32m125\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[43m.\u001B[49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import optuna\n",
    "from optuna.visualization.matplotlib import (\n",
    "    plot_optimization_history,\n",
    "    plot_intermediate_values,\n",
    "    plot_param_importances,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the study from RDB storage\n",
    "storage = optuna.storages.RDBStorage(\"sqlite:///optuna_trials.db\")\n",
    "\n",
    "study = optuna.load_study(study_name=f\"{model_name}-opt-study\", storage=storage)\n",
    "\n",
    "# Plot optimization history\n",
    "ax1 = plot_optimization_history(study)\n",
    "plt.show()\n",
    "ax1.figure.savefig(\"optimization_history.png\")\n",
    "\n",
    "# Plot intermediate values (if using pruning and intermediate reports)\n",
    "ax2 = plot_intermediate_values(study)\n",
    "plt.show()\n",
    "ax2.figure.savefig(\"intermediate_values.png\")\n",
    "\n",
    "# Plot parameter importances\n",
    "ax3 = plot_param_importances(study)\n",
    "plt.show()\n",
    "ax3.figure.savefig(\"param_importances.png\")"
   ],
   "id": "12fa413328ce2cf9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training optimized",
   "id": "6be1a72cf89a009c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "#\n",
    "# # Define the model\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=6)\n",
    "#\n",
    "# # Load best hyperparameters (already defined earlier as best_hparams)\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"./final_model\",\n",
    "#     learning_rate=best_hparams[\"learning_rate\"],\n",
    "#     per_device_train_batch_size=best_hparams[\"per_device_train_batch_size\"],\n",
    "#     weight_decay=best_hparams[\"weight_decay\"],\n",
    "#     eval_strategy=\"epoch\",\n",
    "#     save_strategy=\"epoch\",\n",
    "#     load_best_model_at_end=True,\n",
    "#     logging_strategy=\"epoch\",\n",
    "#     num_train_epochs=best_hparams[\"num_train_epochs\"],\n",
    "#     warmup_steps=best_hparams[\"warmup_steps\"],\n",
    "#     report_to=\"wandb\",\n",
    "#     run_name=\"final_run_with_best_hparams\",\n",
    "# )\n",
    "#\n",
    "# # Create Trainer\n",
    "# trainer = Trainer(\n",
    "#     model=model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=tokenized_datasets[\"train\"],\n",
    "#     eval_dataset=tokenized_datasets[\"valid\"],\n",
    "#     processing_class=tokenizer,\n",
    "#     data_collator=data_collator,\n",
    "#     compute_metrics=compute_metrics,\n",
    "# )\n",
    "# from transformers import EarlyStoppingCallback\n",
    "#\n",
    "# trainer.add_callback(EarlyStoppingCallback(early_stopping_patience=2))\n",
    "# # Train\n",
    "# trainer.train()\n",
    "#\n",
    "# # Save the model\n",
    "# trainer.save_model(\"./final_model\")"
   ],
   "id": "acd20815d3f43391"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# from matplotlib import pyplot as plt\n",
    "# from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "#\n",
    "# # Generate predictions\n",
    "# predictions = trainer.predict(tokenized_datasets[\"test\"])\n",
    "# predicted_labels = predictions.predictions.argmax(axis=-1)\n",
    "#\n",
    "# # Classification report\n",
    "# print(classification_report(tokenized_datasets[\"test\"][\"label\"], predicted_labels))\n",
    "#\n",
    "# # Confusion matrix\n",
    "# cm = confusion_matrix(tokenized_datasets[\"test\"][\"label\"], predicted_labels)\n",
    "# disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"nocb\", \"gender\",\"ethnicity\", \"religion\", \"age\",\"other\"])\n",
    "# disp.plot(cmap=\"Blues\")  # Optional: set a color map\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(\"confusion_matrix.png\", dpi=300)  # You can change the name or dpi as needed\n",
    "# plt.close()  # Close the plot to free memory if you're in a loop"
   ],
   "id": "619f53159586c9f1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# # Inspect misclassified samples\n",
    "# for idx, (pred, label) in enumerate(zip(predicted_labels, tokenized_datasets[\"test\"][\"label\"])):\n",
    "#     if pred != label:\n",
    "#         print(f\"Index: {idx}, Predicted: {pred}, Actual: {label}\")\n",
    "#         print(tokenized_datasets[\"test\"][idx][\"text\"])"
   ],
   "id": "7cd8ae0b19b75b95",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "tokenizer.save_pretrained(\"./final_model\")\n",
    "print(\"Model saved successfully!\")"
   ],
   "id": "c7ad36fbb39a963e",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
